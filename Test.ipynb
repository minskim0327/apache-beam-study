{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM29aDEw85nFZhWuniQgv2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minskim0327/apache-beam-study/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init"
      ],
      "metadata": {
        "id": "ebwRBZjnwkKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBl7desEnPYo",
        "outputId": "d38e80bb-a358-4558-a3b1-a1e5dd2e2d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.52.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7 (from apache-beam)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam)\n",
            "  Downloading fastavro-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.59.3)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (4.19.2)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.23.5)\n",
            "Collecting objsize<0.7.0,>=0.6.1 (from apache-beam)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (23.2)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2023.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (4.5.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam) (10.0.1)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam) (3.1.1)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam) (0.13.2)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam) (2023.11.17)\n",
            "Building wheels for collected packages: crcmod, dill, hdfs, pyjsparser, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=079286dd1730b6f6cbf3f69f9d8e206801a3121018b42fcdcbc62feb5d8fe089\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=00042033dcae802c509596aa4ea509db746a0553eba04ed84bf82ea6dd25528a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=34701ae8110fea75e4674d7e1b8d1720f492b4dfed005e380035d4305cbbcfeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=76dc987ac9f2b1d14201cfede4fb71f2bb630596094f77b4d85b3fb80c74793d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=205317361658b58feb1990b0134283724dc68ce45b0312ba9e1fc23d29a321d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built crcmod dill hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, docopt, crcmod, zstandard, pyarrow-hotfix, orjson, objsize, js2py, fasteners, fastavro, dnspython, dill, pymongo, hdfs, apache-beam\n",
            "Successfully installed apache-beam-2.52.0 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.9.1 fasteners-0.19 hdfs-2.7.3 js2py-0.74 objsize-0.6.1 orjson-3.9.10 pyarrow-hotfix-0.6 pyjsparser-2.7.1 pymongo-4.6.1 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!{'pip install apache-beam'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{'mkdir -p data'}"
      ],
      "metadata": {
        "id": "5ZYwfCt7nmzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0arKajULnt0d",
        "outputId": "3f181519-d2fc-44a9-dd9e-afb4c4314a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XJUlN6R0n2IN",
        "outputId": "0f2a67d8-d5d4-4048-d5ca-a21d235448a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0d9a51b8-b4cc-4803-9843-3a99b34d6af8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0d9a51b8-b4cc-4803-9843-3a99b34d6af8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dept_data.txt to dept_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0HD-Tpore6P",
        "outputId": "73de2fa5-5712-40f9-c512-a7bdda4b5275"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  dept_data.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations in Beam"
      ],
      "metadata": {
        "id": "DMVWCFkfwvtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "  return element.split(',')\n",
        "\n",
        "def filtering(record):\n",
        "  return record[3] == 'Accounts'\n",
        "\n",
        "# with beam.Pipeline() as p1:\n",
        "\n",
        "# Create and give pipeline a name\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    p1\n",
        "     |'Read from file' >> beam.io.ReadFromText('dept_data.txt')  # Initial PCollection by reading data from a source\n",
        "     |'Map transform' >> beam.Map(SplitRow) # Ptransforms according to requirements\n",
        "     |beam.Filter(filtering)\n",
        "     |beam.Map(lambda record: (record[1], 1))\n",
        "     |beam.CombinePerKey(sum) ## CombinePerKey: GroupByKey + Combiner + Reducer\n",
        "     |beam.io.WriteToText('data/output_new') # Write PCollection to a source, Run the pipeline\n",
        "\n",
        ")\n",
        "p1.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/output_new-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bptUwWDhrgsr",
        "outputId": "e7dd0b7b-6a46-4c7e-9c9b-3aaa818bc003"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Marco', 31)\n",
            "('Rebekah', 31)\n",
            "('Itoe', 31)\n",
            "('Edouard', 31)\n",
            "('Kyle', 62)\n",
            "('Kumiko', 31)\n",
            "('Gaston', 31)\n",
            "('Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Branching Pipelines"
      ],
      "metadata": {
        "id": "g1EF8v_X2qgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "\n",
        "input_collection = (\n",
        "                      p\n",
        "                      | \"Read from text file\" >> beam.io.ReadFromText('dept_data.txt')\n",
        "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
        "                   )\n",
        "\n",
        "accounts_count = (\n",
        "                      input_collection\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "                      | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "                 )\n",
        "\n",
        "hr_count = (\n",
        "                input_collection\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "                | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "           )\n",
        "\n",
        "output =(\n",
        "         (accounts_count,hr_count)\n",
        "    | beam.Flatten()\n",
        "    | beam.io.WriteToText('data/both')\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/both-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EFZlsjw2oB6",
        "outputId": "e0c18fe2-b12a-411b-d682-607b02c70ba0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n",
            "('HR, Beryl', 62)\n",
            "('HR, Olga', 31)\n",
            "('HR, Leslie', 31)\n",
            "('HR, Mindy', 31)\n",
            "('HR, Vicky', 31)\n",
            "('HR, Richard', 31)\n",
            "('HR, Kirk', 31)\n",
            "('HR, Kaori', 31)\n",
            "('HR, Oscar', 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Count Assignment"
      ],
      "metadata": {
        "id": "VqNFIsfk4BXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "tcULTods4O7p",
        "outputId": "bd189768-7388-446b-8042-688c532a7c8c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c872509-3925-409a-8854-67c666882f6a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c872509-3925-409a-8854-67c666882f6a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wordcount_data.txt to wordcount_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Transformations are: ReadFromText, Map, FlatMap, CombinePerKey, WriteToText"
      ],
      "metadata": {
        "id": "znjrn_M64vJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "inputs_pattern = 'wordcount_data.txt'\n",
        "outputs_prefix = 'outputs/part'\n",
        "\n",
        "# Running locally in the DirectRunner.\n",
        "pipeline = beam.Pipeline()\n",
        "\n",
        "output = (\n",
        "      pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
        "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
        "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
        "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
        "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
        "\n",
        "  )\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "# run('head -n 20 {}-00000-of-*'.format(outputs_prefix))\n",
        "!{('head -n 20 outputs/part-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk9FZ5_f4aYf",
        "outputId": "9724f96b-2d2b-4633-938c-e2f8a218de95"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('KING', 243)\n",
            "('LEAR', 236)\n",
            "('DRAMATIS', 1)\n",
            "('PERSONAE', 1)\n",
            "('king', 65)\n",
            "('of', 447)\n",
            "('Britain', 2)\n",
            "('OF', 15)\n",
            "('FRANCE', 10)\n",
            "('DUKE', 3)\n",
            "('BURGUNDY', 8)\n",
            "('CORNWALL', 63)\n",
            "('ALBANY', 67)\n",
            "('EARL', 2)\n",
            "('KENT', 156)\n",
            "('GLOUCESTER', 141)\n",
            "('EDGAR', 126)\n",
            "('son', 29)\n",
            "('to', 438)\n",
            "('Gloucester', 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ParDo Transform\n",
        "\n",
        "A ParDo transform takes each element of input Pcollection, performs processing function on it and emits 0, 1, or multiple elements.\n",
        "\n",
        "Functionalities:\n",
        "- Filtering\n",
        "- Formatting or Type Conversion\n",
        "- Extracting individual parts\n",
        "- Computations"
      ],
      "metadata": {
        "id": "uP1rXUUr6qVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class SplitRow(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    return [element.split(',')]\n",
        "\n",
        "class FilterAccountsEmployee(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    if element[3] == 'Accounts':\n",
        "      return [element]\n",
        "\n",
        "class PairEmployees(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    return [(element[3]+\",\"+element[1], 1)]\n",
        "\n",
        "class Counting(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    # return type -> list\n",
        "    (key, values) = element\n",
        "    return [(key, sum(values))]\n",
        "\n",
        "# Create and give pipeline a name\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    p1\n",
        "     |'Read from file' >> beam.io.ReadFromText('dept_data.txt')\n",
        "     |'Map transform' >> beam.ParDo(SplitRow())\n",
        "     |beam.ParDo(FilterAccountsEmployee())\n",
        "     |beam.ParDo(PairEmployees())\n",
        "     |'Group' >> beam.GroupByKey()\n",
        "     |'Sum using ParDo' >> beam.ParDo(Counting())\n",
        "     |beam.io.WriteToText('data/output_new_final') # Write PCollection to a source, Run the pipeline\n",
        "\n",
        ")\n",
        "p1.run()\n",
        "\n",
        "# visualize output\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpjZuHER7Y-M",
        "outputId": "cdcdd418-9dda-4a8b-bf1a-6ac7bdc2fcc4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts,Marco', 31)\n",
            "('Accounts,Rebekah', 31)\n",
            "('Accounts,Itoe', 31)\n",
            "('Accounts,Edouard', 31)\n",
            "('Accounts,Kyle', 62)\n",
            "('Accounts,Kumiko', 31)\n",
            "('Accounts,Gaston', 31)\n",
            "('Accounts,Ayumi', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Combiner of Beam\n",
        "\n",
        "Comibiner is a mini reducer which does the reduce task locally to a mapper machine.\n",
        "\n",
        "List of Combiner methods:\n",
        "\n",
        "- create_accumulator: creates a new local accumulator in each machine. Keep record of (sum, counts)\n",
        "- add_input: Adds an input element to accumulator, returning new (sum, count) value\n",
        "- megre_accumulators: merges all machine accumulators into a single one\n",
        "- extract_ouput: performs the final computation"
      ],
      "metadata": {
        "id": "vPVi-5SU9hwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "class AverageFn(beam.CombineFn):\n",
        "\n",
        "  def create_accumulator(self):\n",
        "     return (0.0, 0)   # initialize (sum, count)\n",
        "\n",
        "  def add_input(self, sum_count, input):\n",
        "    (sum, count) = sum_count\n",
        "    return sum + input, count + 1\n",
        "\n",
        "  def merge_accumulators(self, accumulators):\n",
        "    ind_sums, ind_counts = zip(*accumulators)       # zip - [(27, 3), (39, 3), (18, 2)]  -->   [(27,39,18), (3,3,2)]\n",
        "    return sum(ind_sums), sum(ind_counts)        # (84,8)\n",
        "\n",
        "  def extract_output(self, sum_count):\n",
        "    (sum, count) = sum_count    # combine globally using CombineFn\n",
        "    return sum / count if count else float('NaN')\n",
        "\n",
        "\n",
        "small_sum = (\n",
        "           p\n",
        "            | beam.Create([15,5,7,7,9,23,13,5])\n",
        "            | \"Combine Globally\" >> beam.CombineGlobally(AverageFn())\n",
        "            | 'Write results' >> beam.io.WriteToText('data/combine')\n",
        "          )\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{'head -n 20 data/combine-00000-of-00001'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU7ktNxt-swQ",
        "outputId": "8178c34e-38fe-4c4e-b86b-528cd4529dc8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composite Transforms\n",
        "\n",
        "What if we have repeated trasnformations? Can we group them? Yes!"
      ],
      "metadata": {
        "id": "BsCRd1k6_3S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class MyTransform(beam.PTransform):\n",
        "\n",
        "  def expand(self, input_coll):\n",
        "\n",
        "    a = (\n",
        "        input_coll\n",
        "                       | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "                       | 'count filter accounts' >> beam.Filter(filter_on_count)\n",
        "                       | 'Regular accounts employee' >> beam.Map(format_output)\n",
        "\n",
        "    )\n",
        "    return a\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "\n",
        "\n",
        "def filter_on_count(element):\n",
        "  name, count = element\n",
        "  if count > 30:\n",
        "    return element\n",
        "\n",
        "def format_output(element):\n",
        "  name, count = element\n",
        "  return ((name, str(count), 'Regular employee'))\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "input_collection = (\n",
        "                      p\n",
        "                      | \"Read from text file\" >> beam.io.ReadFromText('dept_data.txt')\n",
        "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
        "                   )\n",
        "\n",
        "accounts_count = (\n",
        "                      input_collection\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "                      | 'Composite accounts' >> MyTransform()\n",
        "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "hr_count = (\n",
        "                input_collection\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "                | 'composite HR' >> MyTransform()\n",
        "                | 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
        "           )\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Account-00000-of-00001')}\n",
        "!{('head -n 20 data/HR-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQrUPxcc_-XY",
        "outputId": "4acc1d1c-2410-4ef6-e5bc-2d830beeb16b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Accounts, Marco', '31', 'Regular employee')\n",
            "('Accounts, Rebekah', '31', 'Regular employee')\n",
            "('Accounts, Itoe', '31', 'Regular employee')\n",
            "('Accounts, Edouard', '31', 'Regular employee')\n",
            "('Accounts, Kyle', '62', 'Regular employee')\n",
            "('Accounts, Kumiko', '31', 'Regular employee')\n",
            "('Accounts, Gaston', '31', 'Regular employee')\n",
            "('HR, Beryl', '62', 'Regular employee')\n",
            "('HR, Olga', '31', 'Regular employee')\n",
            "('HR, Leslie', '31', 'Regular employee')\n",
            "('HR, Mindy', '31', 'Regular employee')\n",
            "('HR, Vicky', '31', 'Regular employee')\n",
            "('HR, Richard', '31', 'Regular employee')\n",
            "('HR, Kirk', '31', 'Regular employee')\n",
            "('HR, Kaori', '31', 'Regular employee')\n",
            "('HR, Oscar', '31', 'Regular employee')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoGroupBy for Joins\n",
        "\n",
        "Relational join of two or more key/value PCollections\n",
        "\n",
        "Accepts a dictionary of key/value PCollections and output a single PCollection containing 1 key/value Tuple for each key in the input PCollections"
      ],
      "metadata": {
        "id": "YfOr9jRJAvEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Py39n3MIBKLq",
        "outputId": "38206fdb-b7fe-4497-d776-a347797bc40f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f06778b0-4927-4db4-95ad-aee3eac6c73d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f06778b0-4927-4db4-95ad-aee3eac6c73d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving location.txt to location.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def retTuple(element):\n",
        "\n",
        "  thisTuple=element.split(',')\n",
        "  return (thisTuple[0],thisTuple[1:])\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "# Apply a ParDo to the PCollection \"words\" to compute lengths for each word.\n",
        "dep_rows = (\n",
        "                p1\n",
        "                | \"Reading File 1\" >> beam.io.ReadFromText('dept_data.txt')\n",
        "                | 'Pair each employee with key' >> beam.Map(retTuple)          # {149633CM : [Marco,10,Accounts,1-01-2019]}\n",
        "\n",
        "               )\n",
        "\n",
        "\n",
        "loc_rows = (\n",
        "                p1\n",
        "                | \"Reading File 2\" >> beam.io.ReadFromText('location.txt')\n",
        "                | 'Pair each loc with key' >> beam.Map(retTuple)                # {149633CM : [9876843261,New York]}\n",
        "               )\n",
        "\n",
        "\n",
        "results = ({'dep_data': dep_rows, 'loc_data': loc_rows}\n",
        "\n",
        "           | beam.CoGroupByKey()\n",
        "           | 'Write results' >> beam.io.WriteToText('data/result')\n",
        "          )\n",
        "\n",
        "\n",
        "p1.run()\n",
        "\n",
        "!{('head -n 20 data/result-00000-of-00001')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Q6n13eBP30",
        "outputId": "037ab3b6-fd70-4c2c-c74e-32eced3da223"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('149633CM', {'dep_data': [['Marco', '10', 'Accounts', '1-01-2019'], ['Marco', '10', 'Accounts', '2-01-2019'], ['Marco', '10', 'Accounts', '3-01-2019'], ['Marco', '10', 'Accounts', '4-01-2019'], ['Marco', '10', 'Accounts', '5-01-2019'], ['Marco', '10', 'Accounts', '6-01-2019'], ['Marco', '10', 'Accounts', '7-01-2019'], ['Marco', '10', 'Accounts', '8-01-2019'], ['Marco', '10', 'Accounts', '9-01-2019'], ['Marco', '10', 'Accounts', '10-01-2019'], ['Marco', '10', 'Accounts', '11-01-2019'], ['Marco', '10', 'Accounts', '12-01-2019'], ['Marco', '10', 'Accounts', '13-01-2019'], ['Marco', '10', 'Accounts', '14-01-2019'], ['Marco', '10', 'Accounts', '15-01-2019'], ['Marco', '10', 'Accounts', '16-01-2019'], ['Marco', '10', 'Accounts', '17-01-2019'], ['Marco', '10', 'Accounts', '18-01-2019'], ['Marco', '10', 'Accounts', '19-01-2019'], ['Marco', '10', 'Accounts', '20-01-2019'], ['Marco', '10', 'Accounts', '21-01-2019'], ['Marco', '10', 'Accounts', '22-01-2019'], ['Marco', '10', 'Accounts', '23-01-2019'], ['Marco', '10', 'Accounts', '24-01-2019'], ['Marco', '10', 'Accounts', '25-01-2019'], ['Marco', '10', 'Accounts', '26-01-2019'], ['Marco', '10', 'Accounts', '27-01-2019'], ['Marco', '10', 'Accounts', '28-01-2019'], ['Marco', '10', 'Accounts', '29-01-2019'], ['Marco', '10', 'Accounts', '30-01-2019'], ['Marco', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9876843261', 'New York'], ['9204232778', 'New York']]})\n",
            "('212539MU', {'dep_data': [['Rebekah', '10', 'Accounts', '1-01-2019'], ['Rebekah', '10', 'Accounts', '2-01-2019'], ['Rebekah', '10', 'Accounts', '3-01-2019'], ['Rebekah', '10', 'Accounts', '4-01-2019'], ['Rebekah', '10', 'Accounts', '5-01-2019'], ['Rebekah', '10', 'Accounts', '6-01-2019'], ['Rebekah', '10', 'Accounts', '7-01-2019'], ['Rebekah', '10', 'Accounts', '8-01-2019'], ['Rebekah', '10', 'Accounts', '9-01-2019'], ['Rebekah', '10', 'Accounts', '10-01-2019'], ['Rebekah', '10', 'Accounts', '11-01-2019'], ['Rebekah', '10', 'Accounts', '12-01-2019'], ['Rebekah', '10', 'Accounts', '13-01-2019'], ['Rebekah', '10', 'Accounts', '14-01-2019'], ['Rebekah', '10', 'Accounts', '15-01-2019'], ['Rebekah', '10', 'Accounts', '16-01-2019'], ['Rebekah', '10', 'Accounts', '17-01-2019'], ['Rebekah', '10', 'Accounts', '18-01-2019'], ['Rebekah', '10', 'Accounts', '19-01-2019'], ['Rebekah', '10', 'Accounts', '20-01-2019'], ['Rebekah', '10', 'Accounts', '21-01-2019'], ['Rebekah', '10', 'Accounts', '22-01-2019'], ['Rebekah', '10', 'Accounts', '23-01-2019'], ['Rebekah', '10', 'Accounts', '24-01-2019'], ['Rebekah', '10', 'Accounts', '25-01-2019'], ['Rebekah', '10', 'Accounts', '26-01-2019'], ['Rebekah', '10', 'Accounts', '27-01-2019'], ['Rebekah', '10', 'Accounts', '28-01-2019'], ['Rebekah', '10', 'Accounts', '29-01-2019'], ['Rebekah', '10', 'Accounts', '30-01-2019'], ['Rebekah', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9995440673', 'Denver']]})\n",
            "('231555ZZ', {'dep_data': [['Itoe', '10', 'Accounts', '1-01-2019'], ['Itoe', '10', 'Accounts', '2-01-2019'], ['Itoe', '10', 'Accounts', '3-01-2019'], ['Itoe', '10', 'Accounts', '4-01-2019'], ['Itoe', '10', 'Accounts', '5-01-2019'], ['Itoe', '10', 'Accounts', '6-01-2019'], ['Itoe', '10', 'Accounts', '7-01-2019'], ['Itoe', '10', 'Accounts', '8-01-2019'], ['Itoe', '10', 'Accounts', '9-01-2019'], ['Itoe', '10', 'Accounts', '10-01-2019'], ['Itoe', '10', 'Accounts', '11-01-2019'], ['Itoe', '10', 'Accounts', '12-01-2019'], ['Itoe', '10', 'Accounts', '13-01-2019'], ['Itoe', '10', 'Accounts', '14-01-2019'], ['Itoe', '10', 'Accounts', '15-01-2019'], ['Itoe', '10', 'Accounts', '16-01-2019'], ['Itoe', '10', 'Accounts', '17-01-2019'], ['Itoe', '10', 'Accounts', '18-01-2019'], ['Itoe', '10', 'Accounts', '19-01-2019'], ['Itoe', '10', 'Accounts', '20-01-2019'], ['Itoe', '10', 'Accounts', '21-01-2019'], ['Itoe', '10', 'Accounts', '22-01-2019'], ['Itoe', '10', 'Accounts', '23-01-2019'], ['Itoe', '10', 'Accounts', '24-01-2019'], ['Itoe', '10', 'Accounts', '25-01-2019'], ['Itoe', '10', 'Accounts', '26-01-2019'], ['Itoe', '10', 'Accounts', '27-01-2019'], ['Itoe', '10', 'Accounts', '28-01-2019'], ['Itoe', '10', 'Accounts', '29-01-2019'], ['Itoe', '10', 'Accounts', '30-01-2019'], ['Itoe', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9196597290', 'Boston']]})\n",
            "('503996WI', {'dep_data': [['Edouard', '10', 'Accounts', '1-01-2019'], ['Edouard', '10', 'Accounts', '2-01-2019'], ['Edouard', '10', 'Accounts', '3-01-2019'], ['Edouard', '10', 'Accounts', '4-01-2019'], ['Edouard', '10', 'Accounts', '5-01-2019'], ['Edouard', '10', 'Accounts', '6-01-2019'], ['Edouard', '10', 'Accounts', '7-01-2019'], ['Edouard', '10', 'Accounts', '8-01-2019'], ['Edouard', '10', 'Accounts', '9-01-2019'], ['Edouard', '10', 'Accounts', '10-01-2019'], ['Edouard', '10', 'Accounts', '11-01-2019'], ['Edouard', '10', 'Accounts', '12-01-2019'], ['Edouard', '10', 'Accounts', '13-01-2019'], ['Edouard', '10', 'Accounts', '14-01-2019'], ['Edouard', '10', 'Accounts', '15-01-2019'], ['Edouard', '10', 'Accounts', '16-01-2019'], ['Edouard', '10', 'Accounts', '17-01-2019'], ['Edouard', '10', 'Accounts', '18-01-2019'], ['Edouard', '10', 'Accounts', '19-01-2019'], ['Edouard', '10', 'Accounts', '20-01-2019'], ['Edouard', '10', 'Accounts', '21-01-2019'], ['Edouard', '10', 'Accounts', '22-01-2019'], ['Edouard', '10', 'Accounts', '23-01-2019'], ['Edouard', '10', 'Accounts', '24-01-2019'], ['Edouard', '10', 'Accounts', '25-01-2019'], ['Edouard', '10', 'Accounts', '26-01-2019'], ['Edouard', '10', 'Accounts', '27-01-2019'], ['Edouard', '10', 'Accounts', '28-01-2019'], ['Edouard', '10', 'Accounts', '29-01-2019'], ['Edouard', '10', 'Accounts', '30-01-2019'], ['Edouard', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9468234252', 'Miami']]})\n",
            "('704275DC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9776235961', 'Miami']]})\n",
            "('957149WC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9925595092', 'Houston']]})\n",
            "('241316NX', {'dep_data': [['Kumiko', '10', 'Accounts', '1-01-2019'], ['Kumiko', '10', 'Accounts', '2-01-2019'], ['Kumiko', '10', 'Accounts', '3-01-2019'], ['Kumiko', '10', 'Accounts', '4-01-2019'], ['Kumiko', '10', 'Accounts', '5-01-2019'], ['Kumiko', '10', 'Accounts', '6-01-2019'], ['Kumiko', '10', 'Accounts', '7-01-2019'], ['Kumiko', '10', 'Accounts', '8-01-2019'], ['Kumiko', '10', 'Accounts', '9-01-2019'], ['Kumiko', '10', 'Accounts', '10-01-2019'], ['Kumiko', '10', 'Accounts', '11-01-2019'], ['Kumiko', '10', 'Accounts', '12-01-2019'], ['Kumiko', '10', 'Accounts', '13-01-2019'], ['Kumiko', '10', 'Accounts', '14-01-2019'], ['Kumiko', '10', 'Accounts', '15-01-2019'], ['Kumiko', '10', 'Accounts', '16-01-2019'], ['Kumiko', '10', 'Accounts', '17-01-2019'], ['Kumiko', '10', 'Accounts', '18-01-2019'], ['Kumiko', '10', 'Accounts', '19-01-2019'], ['Kumiko', '10', 'Accounts', '20-01-2019'], ['Kumiko', '10', 'Accounts', '21-01-2019'], ['Kumiko', '10', 'Accounts', '22-01-2019'], ['Kumiko', '10', 'Accounts', '23-01-2019'], ['Kumiko', '10', 'Accounts', '24-01-2019'], ['Kumiko', '10', 'Accounts', '25-01-2019'], ['Kumiko', '10', 'Accounts', '26-01-2019'], ['Kumiko', '10', 'Accounts', '27-01-2019'], ['Kumiko', '10', 'Accounts', '28-01-2019'], ['Kumiko', '10', 'Accounts', '29-01-2019'], ['Kumiko', '10', 'Accounts', '30-01-2019'], ['Kumiko', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9837402343', 'Boston']]})\n",
            "('796656IE', {'dep_data': [['Gaston', '10', 'Accounts', '1-01-2019'], ['Gaston', '10', 'Accounts', '2-01-2019'], ['Gaston', '10', 'Accounts', '3-01-2019'], ['Gaston', '10', 'Accounts', '4-01-2019'], ['Gaston', '10', 'Accounts', '5-01-2019'], ['Gaston', '10', 'Accounts', '6-01-2019'], ['Gaston', '10', 'Accounts', '7-01-2019'], ['Gaston', '10', 'Accounts', '8-01-2019'], ['Gaston', '10', 'Accounts', '9-01-2019'], ['Gaston', '10', 'Accounts', '10-01-2019'], ['Gaston', '10', 'Accounts', '11-01-2019'], ['Gaston', '10', 'Accounts', '12-01-2019'], ['Gaston', '10', 'Accounts', '13-01-2019'], ['Gaston', '10', 'Accounts', '14-01-2019'], ['Gaston', '10', 'Accounts', '15-01-2019'], ['Gaston', '10', 'Accounts', '16-01-2019'], ['Gaston', '10', 'Accounts', '17-01-2019'], ['Gaston', '10', 'Accounts', '18-01-2019'], ['Gaston', '10', 'Accounts', '19-01-2019'], ['Gaston', '10', 'Accounts', '20-01-2019'], ['Gaston', '10', 'Accounts', '21-01-2019'], ['Gaston', '10', 'Accounts', '22-01-2019'], ['Gaston', '10', 'Accounts', '23-01-2019'], ['Gaston', '10', 'Accounts', '24-01-2019'], ['Gaston', '10', 'Accounts', '25-01-2019'], ['Gaston', '10', 'Accounts', '26-01-2019'], ['Gaston', '10', 'Accounts', '27-01-2019'], ['Gaston', '10', 'Accounts', '28-01-2019'], ['Gaston', '10', 'Accounts', '29-01-2019'], ['Gaston', '10', 'Accounts', '30-01-2019'], ['Gaston', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9538848876', 'Houston']]})\n",
            "('718737IX', {'dep_data': [['Ayumi', '10', 'Accounts', '2-01-2019'], ['Ayumi', '10', 'Accounts', '3-01-2019'], ['Ayumi', '10', 'Accounts', '4-01-2019'], ['Ayumi', '10', 'Accounts', '5-01-2019'], ['Ayumi', '10', 'Accounts', '6-01-2019'], ['Ayumi', '10', 'Accounts', '7-01-2019'], ['Ayumi', '10', 'Accounts', '8-01-2019'], ['Ayumi', '10', 'Accounts', '9-01-2019'], ['Ayumi', '10', 'Accounts', '10-01-2019'], ['Ayumi', '10', 'Accounts', '11-01-2019'], ['Ayumi', '10', 'Accounts', '12-01-2019'], ['Ayumi', '10', 'Accounts', '13-01-2019'], ['Ayumi', '10', 'Accounts', '14-01-2019'], ['Ayumi', '10', 'Accounts', '15-01-2019'], ['Ayumi', '10', 'Accounts', '16-01-2019'], ['Ayumi', '10', 'Accounts', '17-01-2019'], ['Ayumi', '10', 'Accounts', '18-01-2019'], ['Ayumi', '10', 'Accounts', '19-01-2019'], ['Ayumi', '10', 'Accounts', '20-01-2019'], ['Ayumi', '10', 'Accounts', '21-01-2019'], ['Ayumi', '10', 'Accounts', '22-01-2019'], ['Ayumi', '10', 'Accounts', '23-01-2019'], ['Ayumi', '10', 'Accounts', '24-01-2019'], ['Ayumi', '10', 'Accounts', '25-01-2019'], ['Ayumi', '10', 'Accounts', '26-01-2019'], ['Ayumi', '10', 'Accounts', '27-01-2019'], ['Ayumi', '10', 'Accounts', '28-01-2019'], ['Ayumi', '10', 'Accounts', '29-01-2019'], ['Ayumi', '10', 'Accounts', '30-01-2019'], ['Ayumi', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9204232788', 'Austin']]})\n",
            "('331593PS', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9137216186', 'Miami']]})\n",
            "('560447WH', {'dep_data': [['Olga', '20', 'HR', '1-01-2019'], ['Olga', '20', 'HR', '2-01-2019'], ['Olga', '20', 'HR', '3-01-2019'], ['Olga', '20', 'HR', '4-01-2019'], ['Olga', '20', 'HR', '5-01-2019'], ['Olga', '20', 'HR', '6-01-2019'], ['Olga', '20', 'HR', '7-01-2019'], ['Olga', '20', 'HR', '8-01-2019'], ['Olga', '20', 'HR', '9-01-2019'], ['Olga', '20', 'HR', '10-01-2019'], ['Olga', '20', 'HR', '11-01-2019'], ['Olga', '20', 'HR', '12-01-2019'], ['Olga', '20', 'HR', '13-01-2019'], ['Olga', '20', 'HR', '14-01-2019'], ['Olga', '20', 'HR', '15-01-2019'], ['Olga', '20', 'HR', '16-01-2019'], ['Olga', '20', 'HR', '17-01-2019'], ['Olga', '20', 'HR', '18-01-2019'], ['Olga', '20', 'HR', '19-01-2019'], ['Olga', '20', 'HR', '20-01-2019'], ['Olga', '20', 'HR', '21-01-2019'], ['Olga', '20', 'HR', '22-01-2019'], ['Olga', '20', 'HR', '23-01-2019'], ['Olga', '20', 'HR', '24-01-2019'], ['Olga', '20', 'HR', '25-01-2019'], ['Olga', '20', 'HR', '26-01-2019'], ['Olga', '20', 'HR', '27-01-2019'], ['Olga', '20', 'HR', '28-01-2019'], ['Olga', '20', 'HR', '29-01-2019'], ['Olga', '20', 'HR', '30-01-2019'], ['Olga', '20', 'HR', '31-01-2019']], 'loc_data': [['9708010864', 'Miami']]})\n",
            "('222997TJ', {'dep_data': [['Leslie', '20', 'HR', '1-01-2019'], ['Leslie', '20', 'HR', '2-01-2019'], ['Leslie', '20', 'HR', '3-01-2019'], ['Leslie', '20', 'HR', '4-01-2019'], ['Leslie', '20', 'HR', '5-01-2019'], ['Leslie', '20', 'HR', '6-01-2019'], ['Leslie', '20', 'HR', '7-01-2019'], ['Leslie', '20', 'HR', '8-01-2019'], ['Leslie', '20', 'HR', '9-01-2019'], ['Leslie', '20', 'HR', '10-01-2019'], ['Leslie', '20', 'HR', '11-01-2019'], ['Leslie', '20', 'HR', '12-01-2019'], ['Leslie', '20', 'HR', '13-01-2019'], ['Leslie', '20', 'HR', '14-01-2019'], ['Leslie', '20', 'HR', '15-01-2019'], ['Leslie', '20', 'HR', '16-01-2019'], ['Leslie', '20', 'HR', '17-01-2019'], ['Leslie', '20', 'HR', '18-01-2019'], ['Leslie', '20', 'HR', '19-01-2019'], ['Leslie', '20', 'HR', '20-01-2019'], ['Leslie', '20', 'HR', '21-01-2019'], ['Leslie', '20', 'HR', '22-01-2019'], ['Leslie', '20', 'HR', '23-01-2019'], ['Leslie', '20', 'HR', '24-01-2019'], ['Leslie', '20', 'HR', '25-01-2019'], ['Leslie', '20', 'HR', '26-01-2019'], ['Leslie', '20', 'HR', '27-01-2019'], ['Leslie', '20', 'HR', '28-01-2019'], ['Leslie', '20', 'HR', '29-01-2019'], ['Leslie', '20', 'HR', '30-01-2019'], ['Leslie', '20', 'HR', '31-01-2019']], 'loc_data': [['9410006713', 'Washington']]})\n",
            "('171752SY', {'dep_data': [['Mindy', '20', 'HR', '1-01-2019'], ['Mindy', '20', 'HR', '2-01-2019'], ['Mindy', '20', 'HR', '3-01-2019'], ['Mindy', '20', 'HR', '4-01-2019'], ['Mindy', '20', 'HR', '5-01-2019'], ['Mindy', '20', 'HR', '6-01-2019'], ['Mindy', '20', 'HR', '7-01-2019'], ['Mindy', '20', 'HR', '8-01-2019'], ['Mindy', '20', 'HR', '9-01-2019'], ['Mindy', '20', 'HR', '10-01-2019'], ['Mindy', '20', 'HR', '11-01-2019'], ['Mindy', '20', 'HR', '12-01-2019'], ['Mindy', '20', 'HR', '13-01-2019'], ['Mindy', '20', 'HR', '14-01-2019'], ['Mindy', '20', 'HR', '15-01-2019'], ['Mindy', '20', 'HR', '16-01-2019'], ['Mindy', '20', 'HR', '17-01-2019'], ['Mindy', '20', 'HR', '18-01-2019'], ['Mindy', '20', 'HR', '19-01-2019'], ['Mindy', '20', 'HR', '20-01-2019'], ['Mindy', '20', 'HR', '21-01-2019'], ['Mindy', '20', 'HR', '22-01-2019'], ['Mindy', '20', 'HR', '23-01-2019'], ['Mindy', '20', 'HR', '24-01-2019'], ['Mindy', '20', 'HR', '25-01-2019'], ['Mindy', '20', 'HR', '26-01-2019'], ['Mindy', '20', 'HR', '27-01-2019'], ['Mindy', '20', 'HR', '28-01-2019'], ['Mindy', '20', 'HR', '29-01-2019'], ['Mindy', '20', 'HR', '30-01-2019'], ['Mindy', '20', 'HR', '31-01-2019']], 'loc_data': [['9494683837', 'Boston']]})\n",
            "('153636AS', {'dep_data': [['Vicky', '20', 'HR', '1-01-2019'], ['Vicky', '20', 'HR', '2-01-2019'], ['Vicky', '20', 'HR', '3-01-2019'], ['Vicky', '20', 'HR', '4-01-2019'], ['Vicky', '20', 'HR', '5-01-2019'], ['Vicky', '20', 'HR', '6-01-2019'], ['Vicky', '20', 'HR', '7-01-2019'], ['Vicky', '20', 'HR', '8-01-2019'], ['Vicky', '20', 'HR', '9-01-2019'], ['Vicky', '20', 'HR', '10-01-2019'], ['Vicky', '20', 'HR', '11-01-2019'], ['Vicky', '20', 'HR', '12-01-2019'], ['Vicky', '20', 'HR', '13-01-2019'], ['Vicky', '20', 'HR', '14-01-2019'], ['Vicky', '20', 'HR', '15-01-2019'], ['Vicky', '20', 'HR', '16-01-2019'], ['Vicky', '20', 'HR', '17-01-2019'], ['Vicky', '20', 'HR', '18-01-2019'], ['Vicky', '20', 'HR', '19-01-2019'], ['Vicky', '20', 'HR', '20-01-2019'], ['Vicky', '20', 'HR', '21-01-2019'], ['Vicky', '20', 'HR', '22-01-2019'], ['Vicky', '20', 'HR', '23-01-2019'], ['Vicky', '20', 'HR', '24-01-2019'], ['Vicky', '20', 'HR', '25-01-2019'], ['Vicky', '20', 'HR', '26-01-2019'], ['Vicky', '20', 'HR', '27-01-2019'], ['Vicky', '20', 'HR', '28-01-2019'], ['Vicky', '20', 'HR', '29-01-2019'], ['Vicky', '20', 'HR', '30-01-2019'], ['Vicky', '20', 'HR', '31-01-2019']], 'loc_data': [['9575378417', 'New York']]})\n",
            "('745411HT', {'dep_data': [['Richard', '20', 'HR', '1-01-2019'], ['Richard', '20', 'HR', '2-01-2019'], ['Richard', '20', 'HR', '3-01-2019'], ['Richard', '20', 'HR', '4-01-2019'], ['Richard', '20', 'HR', '5-01-2019'], ['Richard', '20', 'HR', '6-01-2019'], ['Richard', '20', 'HR', '7-01-2019'], ['Richard', '20', 'HR', '8-01-2019'], ['Richard', '20', 'HR', '9-01-2019'], ['Richard', '20', 'HR', '10-01-2019'], ['Richard', '20', 'HR', '11-01-2019'], ['Richard', '20', 'HR', '12-01-2019'], ['Richard', '20', 'HR', '13-01-2019'], ['Richard', '20', 'HR', '14-01-2019'], ['Richard', '20', 'HR', '15-01-2019'], ['Richard', '20', 'HR', '16-01-2019'], ['Richard', '20', 'HR', '17-01-2019'], ['Richard', '20', 'HR', '18-01-2019'], ['Richard', '20', 'HR', '19-01-2019'], ['Richard', '20', 'HR', '20-01-2019'], ['Richard', '20', 'HR', '21-01-2019'], ['Richard', '20', 'HR', '22-01-2019'], ['Richard', '20', 'HR', '23-01-2019'], ['Richard', '20', 'HR', '24-01-2019'], ['Richard', '20', 'HR', '25-01-2019'], ['Richard', '20', 'HR', '26-01-2019'], ['Richard', '20', 'HR', '27-01-2019'], ['Richard', '20', 'HR', '28-01-2019'], ['Richard', '20', 'HR', '29-01-2019'], ['Richard', '20', 'HR', '30-01-2019'], ['Richard', '20', 'HR', '31-01-2019']], 'loc_data': [['9391632080', 'New York']]})\n",
            "('298464HN', {'dep_data': [['Kirk', '20', 'HR', '1-01-2019'], ['Kirk', '20', 'HR', '2-01-2019'], ['Kirk', '20', 'HR', '3-01-2019'], ['Kirk', '20', 'HR', '4-01-2019'], ['Kirk', '20', 'HR', '5-01-2019'], ['Kirk', '20', 'HR', '6-01-2019'], ['Kirk', '20', 'HR', '7-01-2019'], ['Kirk', '20', 'HR', '8-01-2019'], ['Kirk', '20', 'HR', '9-01-2019'], ['Kirk', '20', 'HR', '10-01-2019'], ['Kirk', '20', 'HR', '11-01-2019'], ['Kirk', '20', 'HR', '12-01-2019'], ['Kirk', '20', 'HR', '13-01-2019'], ['Kirk', '20', 'HR', '14-01-2019'], ['Kirk', '20', 'HR', '15-01-2019'], ['Kirk', '20', 'HR', '16-01-2019'], ['Kirk', '20', 'HR', '17-01-2019'], ['Kirk', '20', 'HR', '18-01-2019'], ['Kirk', '20', 'HR', '19-01-2019'], ['Kirk', '20', 'HR', '20-01-2019'], ['Kirk', '20', 'HR', '21-01-2019'], ['Kirk', '20', 'HR', '22-01-2019'], ['Kirk', '20', 'HR', '23-01-2019'], ['Kirk', '20', 'HR', '24-01-2019'], ['Kirk', '20', 'HR', '25-01-2019'], ['Kirk', '20', 'HR', '26-01-2019'], ['Kirk', '20', 'HR', '27-01-2019'], ['Kirk', '20', 'HR', '28-01-2019'], ['Kirk', '20', 'HR', '29-01-2019'], ['Kirk', '20', 'HR', '30-01-2019'], ['Kirk', '20', 'HR', '31-01-2019']], 'loc_data': [['9982641601', 'Miami']]})\n",
            "('783950BW', {'dep_data': [['Kaori', '20', 'HR', '1-01-2019'], ['Kaori', '20', 'HR', '2-01-2019'], ['Kaori', '20', 'HR', '3-01-2019'], ['Kaori', '20', 'HR', '4-01-2019'], ['Kaori', '20', 'HR', '5-01-2019'], ['Kaori', '20', 'HR', '6-01-2019'], ['Kaori', '20', 'HR', '7-01-2019'], ['Kaori', '20', 'HR', '8-01-2019'], ['Kaori', '20', 'HR', '9-01-2019'], ['Kaori', '20', 'HR', '10-01-2019'], ['Kaori', '20', 'HR', '11-01-2019'], ['Kaori', '20', 'HR', '12-01-2019'], ['Kaori', '20', 'HR', '13-01-2019'], ['Kaori', '20', 'HR', '14-01-2019'], ['Kaori', '20', 'HR', '15-01-2019'], ['Kaori', '20', 'HR', '16-01-2019'], ['Kaori', '20', 'HR', '17-01-2019'], ['Kaori', '20', 'HR', '18-01-2019'], ['Kaori', '20', 'HR', '19-01-2019'], ['Kaori', '20', 'HR', '20-01-2019'], ['Kaori', '20', 'HR', '21-01-2019'], ['Kaori', '20', 'HR', '22-01-2019'], ['Kaori', '20', 'HR', '23-01-2019'], ['Kaori', '20', 'HR', '24-01-2019'], ['Kaori', '20', 'HR', '25-01-2019'], ['Kaori', '20', 'HR', '26-01-2019'], ['Kaori', '20', 'HR', '27-01-2019'], ['Kaori', '20', 'HR', '28-01-2019'], ['Kaori', '20', 'HR', '29-01-2019'], ['Kaori', '20', 'HR', '30-01-2019'], ['Kaori', '20', 'HR', '31-01-2019']], 'loc_data': [['9248480224', 'Boston']]})\n",
            "('892691AR', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9723803710', 'New York']]})\n",
            "('245668UZ', {'dep_data': [['Oscar', '20', 'HR', '1-01-2019'], ['Oscar', '20', 'HR', '2-01-2019'], ['Oscar', '20', 'HR', '3-01-2019'], ['Oscar', '20', 'HR', '4-01-2019'], ['Oscar', '20', 'HR', '5-01-2019'], ['Oscar', '20', 'HR', '6-01-2019'], ['Oscar', '20', 'HR', '7-01-2019'], ['Oscar', '20', 'HR', '8-01-2019'], ['Oscar', '20', 'HR', '9-01-2019'], ['Oscar', '20', 'HR', '10-01-2019'], ['Oscar', '20', 'HR', '11-01-2019'], ['Oscar', '20', 'HR', '12-01-2019'], ['Oscar', '20', 'HR', '13-01-2019'], ['Oscar', '20', 'HR', '14-01-2019'], ['Oscar', '20', 'HR', '15-01-2019'], ['Oscar', '20', 'HR', '16-01-2019'], ['Oscar', '20', 'HR', '17-01-2019'], ['Oscar', '20', 'HR', '18-01-2019'], ['Oscar', '20', 'HR', '19-01-2019'], ['Oscar', '20', 'HR', '20-01-2019'], ['Oscar', '20', 'HR', '21-01-2019'], ['Oscar', '20', 'HR', '22-01-2019'], ['Oscar', '20', 'HR', '23-01-2019'], ['Oscar', '20', 'HR', '24-01-2019'], ['Oscar', '20', 'HR', '25-01-2019'], ['Oscar', '20', 'HR', '26-01-2019'], ['Oscar', '20', 'HR', '27-01-2019'], ['Oscar', '20', 'HR', '28-01-2019'], ['Oscar', '20', 'HR', '29-01-2019'], ['Oscar', '20', 'HR', '30-01-2019'], ['Oscar', '20', 'HR', '31-01-2019']], 'loc_data': [['9395037841', 'New York']]})\n",
            "('231206QD', {'dep_data': [['Kumiko', '30', 'Finance', '1-01-2019'], ['Kumiko', '30', 'Finance', '2-01-2019'], ['Kumiko', '30', 'Finance', '3-01-2019'], ['Kumiko', '30', 'Finance', '4-01-2019'], ['Kumiko', '30', 'Finance', '5-01-2019'], ['Kumiko', '30', 'Finance', '6-01-2019'], ['Kumiko', '30', 'Finance', '7-01-2019'], ['Kumiko', '30', 'Finance', '8-01-2019'], ['Kumiko', '30', 'Finance', '9-01-2019'], ['Kumiko', '30', 'Finance', '10-01-2019'], ['Kumiko', '30', 'Finance', '11-01-2019'], ['Kumiko', '30', 'Finance', '12-01-2019'], ['Kumiko', '30', 'Finance', '13-01-2019'], ['Kumiko', '30', 'Finance', '14-01-2019'], ['Kumiko', '30', 'Finance', '15-01-2019'], ['Kumiko', '30', 'Finance', '16-01-2019'], ['Kumiko', '30', 'Finance', '17-01-2019'], ['Kumiko', '30', 'Finance', '18-01-2019'], ['Kumiko', '30', 'Finance', '19-01-2019'], ['Kumiko', '30', 'Finance', '20-01-2019'], ['Kumiko', '30', 'Finance', '21-01-2019'], ['Kumiko', '30', 'Finance', '22-01-2019'], ['Kumiko', '30', 'Finance', '23-01-2019'], ['Kumiko', '30', 'Finance', '24-01-2019'], ['Kumiko', '30', 'Finance', '25-01-2019'], ['Kumiko', '30', 'Finance', '26-01-2019'], ['Kumiko', '30', 'Finance', '27-01-2019'], ['Kumiko', '30', 'Finance', '28-01-2019'], ['Kumiko', '30', 'Finance', '29-01-2019'], ['Kumiko', '30', 'Finance', '30-01-2019'], ['Kumiko', '30', 'Finance', '31-01-2019']], 'loc_data': [['9608996582', 'Chicago']]})\n"
          ]
        }
      ]
    }
  ]
}